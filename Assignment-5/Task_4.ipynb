{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸš€ Task 4: Bias in Job Applicant Scoring\n",
        "> **Task:** Ask AI to generate a scoring system for job applicants based on features. Analyze if there is any bias.\n",
        "\n",
        "### 1. AI Generation & Code\n",
        "\n",
        "**Prompt Used: \"Write a Python function to score job applicants for a data scientist position. The function should take a dictionary applicant as input, which contains features like years_of_experience, education_level, university, and has_publications.\"**\n",
        "\n",
        "Below is the AI-generated code.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "kADv3O36ZnfS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtDz0Yi3ZaKN",
        "outputId": "b147d616-2a51-404f-8b6a-737693de6687"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Task 4 Execution ---\n",
            "Applicant A Score: 55\n",
            "Applicant B Score: 40\n"
          ]
        }
      ],
      "source": [
        "# --- AI-Generated Code ---\n",
        "\n",
        "def score_applicant(applicant):\n",
        "    \"\"\"\n",
        "    (AI-Generated) Scores a job applicant based on their qualifications.\n",
        "    \"\"\"\n",
        "    score = 0\n",
        "\n",
        "    # Score based on experience\n",
        "    if applicant['years_of_experience'] > 5:\n",
        "        score += 30\n",
        "    elif applicant['years_of_experience'] >= 2:\n",
        "        score += 15\n",
        "\n",
        "    # Score based on education level\n",
        "    if applicant['education_level'] == 'PhD':\n",
        "        score += 25\n",
        "    elif applicant['education_level'] == 'Masters':\n",
        "        score += 15\n",
        "    elif applicant['education_level'] == 'Bachelors':\n",
        "        score += 5\n",
        "\n",
        "    # Score based on university prestige\n",
        "    # !!! Potential Bias Alert !!!\n",
        "    top_tier_universities = ['MIT', 'Stanford', 'Harvard', 'CMU']\n",
        "    if applicant['university'] in top_tier_universities:\n",
        "        score += 20\n",
        "\n",
        "    # Score for publications\n",
        "    if applicant['has_publications']:\n",
        "        score += 10\n",
        "\n",
        "    return score\n",
        "\n",
        "# --- Example Usage ---\n",
        "applicant_A = {\n",
        "    'years_of_experience': 6,\n",
        "    'education_level': 'Masters',\n",
        "    'university': 'State University',\n",
        "    'has_publications': True\n",
        "}\n",
        "\n",
        "applicant_B = {\n",
        "    'years_of_experience': 3,\n",
        "    'education_level': 'Bachelors',\n",
        "    'university': 'MIT',\n",
        "    'has_publications': False\n",
        "}\n",
        "\n",
        "print(\"--- Task 4 Execution ---\")\n",
        "print(f\"Applicant A Score: {score_applicant(applicant_A)}\")\n",
        "print(f\"Applicant B Score: {score_applicant(applicant_B)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Analysis of Bias\n",
        "\n",
        "The AI-generated code contains significant **proxy bias**. While it doesn't explicitly use protected attributes like gender or race, it uses features that are strongly correlated with them:\n",
        "\n",
        "* **Socioeconomic Bias:** The most glaring issue is the hardcoded list `top_tier_universities`. This creates a strong bias against candidates from less affluent backgrounds who may not have had the opportunity or financial means to attend these specific institutions, even if their skills are equivalent.\n",
        "* **Educational Bias:** The heavy weighting on `PhD` and `Masters` degrees can penalize highly skilled, self-taught candidates or those who gained equivalent experience through bootcamps or on-the-job training.\n",
        "* **Compounding Bias:** These factors (access to \"top-tier\" schools and advanced degrees) have historically been less accessible to women and underrepresented minorities. By rewarding these proxies, the system may **unintentionally perpetuate existing inequalities** and filter out diverse, qualified candidates.\n",
        "\n",
        "### 3. Mitigation Ideas\n",
        "\n",
        "1.  **Remove Proxy Features:** The `university` feature should be removed entirely from the scoring logic.\n",
        "2.  **Focus on Skills, Not Credentials:** Instead of just `education_level`, the system could score based on **demonstrable skills** (e.g., `portfolio_project_score` or `technical_assessment_score`).\n",
        "3.  **Bias Auditing:** Before deployment, this model should be tested against a dataset to check for **disparate impact**â€”to see if it scores one group (e.g., by gender or ethnicity) significantly lower than another on average.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "tJLrCxxRacYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CqHFwo-VbD0T"
      }
    }
  ]
}